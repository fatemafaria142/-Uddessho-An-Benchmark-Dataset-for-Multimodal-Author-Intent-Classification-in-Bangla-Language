{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-23T08:04:27.273780Z","iopub.status.busy":"2024-06-23T08:04:27.273169Z","iopub.status.idle":"2024-06-23T08:04:27.339484Z","shell.execute_reply":"2024-06-23T08:04:27.338600Z","shell.execute_reply.started":"2024-06-23T08:04:27.273743Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertTokenizer, BertModel, AdamW\n","import torchvision.models as models\n","from torch import nn\n","import time\n","from tqdm import tqdm\n","\n","# Paths to the CSV files and image directories\n","csv_paths = {\n","    'train': '/kaggle/input/intent/Intent/train.csv',\n","    'test': '/kaggle/input/intent/Intent/validation.csv',\n","    'validation': '/kaggle/input/intent/Intent/validation.csv'\n","}\n","\n","image_dirs = {\n","    'train': '/kaggle/input/intent/Intent/train',\n","    'test': '/kaggle/input/intent/Intent/validation',\n","    'validation': '/kaggle/input/intent/Intent/validation'\n","}\n","output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n","\n","# Function to check for matching Meme_ID and image files, and add image paths\n","def check_matches(csv_path, image_dir):\n","    df = pd.read_csv(csv_path)\n","    image_files = os.listdir(image_dir)\n","    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n","    \n","    # Add Image_Path column to the dataframe\n","    df['Image_Path'] = df['Image_ID'].apply(lambda x: image_names.get(x, None))\n","    \n","    # Filter rows where Image_Path is not None (i.e., matched Meme_IDs)\n","    matched_df = df[df['Image_Path'].notna()]\n","    \n","    return matched_df\n","\n","# Function to encode Intent_Taxonomy classes into labels\n","def encode_labels(df):\n","    label_encoder = LabelEncoder()\n","    df['Intent_Taxonomy_Labels'] = label_encoder.fit_transform(df['Intent_Taxonomy'])\n","    return df, label_encoder.classes_\n","\n","# Check matches for each set (Train, Test, Validation)\n","for key in csv_paths:\n","    matched_df = check_matches(csv_paths[key], image_dirs[key])\n","    \n","    # Encode Intent_Taxonomy labels\n","    matched_df, classes = encode_labels(matched_df)\n","    \n","    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n","    \n","    # Save the processed dataframe to CSV\n","    matched_df.to_csv(matches_output_path, index=False)\n","    \n","    print(f\"{key} set:\")\n","    print(f\"Matched Meme_IDs with image paths and labels saved to {matches_output_path}\")\n","    print(f\"Classes and their corresponding labels:\\n{dict(zip(classes, range(len(classes))))}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:04:30.659860Z","iopub.status.busy":"2024-06-23T08:04:30.659117Z","iopub.status.idle":"2024-06-23T08:04:30.674412Z","shell.execute_reply":"2024-06-23T08:04:30.673476Z","shell.execute_reply.started":"2024-06-23T08:04:30.659826Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/train_matches.csv')\n","train_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:04:32.775915Z","iopub.status.busy":"2024-06-23T08:04:32.775037Z","iopub.status.idle":"2024-06-23T08:04:32.791252Z","shell.execute_reply":"2024-06-23T08:04:32.789992Z","shell.execute_reply.started":"2024-06-23T08:04:32.775874Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/working/test_matches.csv')\n","test_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:04:35.698871Z","iopub.status.busy":"2024-06-23T08:04:35.698062Z","iopub.status.idle":"2024-06-23T08:04:35.713035Z","shell.execute_reply":"2024-06-23T08:04:35.712195Z","shell.execute_reply.started":"2024-06-23T08:04:35.698837Z"},"trusted":true},"outputs":[],"source":["validation_df = pd.read_csv('/kaggle/working/validation_matches.csv')\n","validation_df.head(10)"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:04:38.034799Z","iopub.status.busy":"2024-06-23T08:04:38.033999Z","iopub.status.idle":"2024-06-23T08:04:38.042756Z","shell.execute_reply":"2024-06-23T08:04:38.041829Z","shell.execute_reply.started":"2024-06-23T08:04:38.034765Z"},"trusted":true},"outputs":[],"source":["# Define your transformations using transforms.Compose\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Crop the center to 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","class MyMultimodalDataset(Dataset):\n","    def __init__(self, image_paths, image_captions, intent_taxonomy_labels, transform=None):\n","        self.image_paths = image_paths\n","        self.image_captions = image_captions\n","        self.intent_taxonomy = intent_taxonomy_labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        text = self.image_captions[idx]\n","        label = self.intent_taxonomy[idx]\n","\n","        # Load and preprocess image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, text, label"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:04:41.197817Z","iopub.status.busy":"2024-06-23T08:04:41.197088Z","iopub.status.idle":"2024-06-23T08:04:41.206225Z","shell.execute_reply":"2024-06-23T08:04:41.205246Z","shell.execute_reply.started":"2024-06-23T08:04:41.197777Z"},"trusted":true},"outputs":[],"source":["# Assuming you have lists or arrays of image paths, captions, and encoded labels:\n","train_dataset = MyMultimodalDataset(train_df['Image_Path'], train_df['Image_Caption'], train_df['Intent_Taxonomy_Labels'], transform=transform)\n","val_dataset = MyMultimodalDataset(validation_df['Image_Path'], validation_df['Image_Caption'], validation_df['Intent_Taxonomy_Labels'], transform=transform)\n","test_dataset = MyMultimodalDataset(test_df['Image_Path'], test_df['Image_Caption'], test_df['Intent_Taxonomy_Labels'], transform=transform)\n","\n","# Define data loaders\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:04:43.142764Z","iopub.status.busy":"2024-06-23T08:04:43.141997Z","iopub.status.idle":"2024-06-23T08:04:43.147706Z","shell.execute_reply":"2024-06-23T08:04:43.146802Z","shell.execute_reply.started":"2024-06-23T08:04:43.142730Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertModel, BertTokenizer,AdamW\n","from tqdm import tqdm\n","import torchvision.models as models\n","import time\n","from torchvision.models import densenet169"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:04:45.097682Z","iopub.status.busy":"2024-06-23T08:04:45.096826Z","iopub.status.idle":"2024-06-23T08:04:45.102851Z","shell.execute_reply":"2024-06-23T08:04:45.101912Z","shell.execute_reply.started":"2024-06-23T08:04:45.097647Z"},"trusted":true},"outputs":[],"source":["# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:16:31.701676Z","iopub.status.busy":"2024-06-23T08:16:31.700811Z","iopub.status.idle":"2024-06-23T08:16:31.706218Z","shell.execute_reply":"2024-06-23T08:16:31.705273Z","shell.execute_reply.started":"2024-06-23T08:16:31.701631Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","from torch.optim import AdamW\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:16:34.587914Z","iopub.status.busy":"2024-06-23T08:16:34.587066Z","iopub.status.idle":"2024-06-23T08:16:34.596877Z","shell.execute_reply":"2024-06-23T08:16:34.595999Z","shell.execute_reply.started":"2024-06-23T08:16:34.587882Z"},"trusted":true},"outputs":[],"source":["# Define optimizer and loss function\n","optimizer = AdamW(list(densenet169.parameters()) + list(bert_model.parameters()), lr=2e-5)\n","criterion = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:19:55.409849Z","iopub.status.busy":"2024-06-23T08:19:55.409368Z","iopub.status.idle":"2024-06-23T08:20:01.201238Z","shell.execute_reply":"2024-06-23T08:20:01.200310Z","shell.execute_reply.started":"2024-06-23T08:19:55.409813Z"},"trusted":true},"outputs":[],"source":["# Define the densenet169Features class\n","class densenet169Features(nn.Module):\n","    def __init__(self, original_model):\n","        super(densenet169Features, self).__init__()\n","        self.features = original_model.features\n","        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.pooling(x)\n","        x = torch.flatten(x, 1)\n","        return x\n","\n","# Initialize densenet169_model with IMAGENET1K_V1 weights\n","densenet169 = models.densenet169(weights='IMAGENET1K_V1', progress=True)\n","densenet169 = densenet169Features(densenet169)\n","\n","# Initialize BERT tokenizer and model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","# Define the EarlyFusionModel class\n","class EarlyFusionModel(nn.Module):\n","    def __init__(self, densenet, bert_model, num_classes):\n","        super(EarlyFusionModel, self).__init__()\n","        self.densenet = densenet\n","        self.bert_model = bert_model\n","        self.fc = nn.Sequential(\n","            nn.Linear(1664 + 768, 512),  # Updated input dimension to match concatenated features\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, images, input_ids, attention_mask):\n","        # Image features\n","        img_features = self.densenet(images)\n","        # Ensure img_features shape is correct\n","        assert img_features.shape[1] == 1664, f\"Unexpected img_features shape: {img_features.shape}\"\n","\n","        # Text features\n","        bert_outputs = self.bert_model(input_ids, attention_mask=attention_mask)\n","        text_features = bert_outputs.last_hidden_state[:, 0, :]\n","\n","        # Ensure text_features shape is correct\n","        assert text_features.shape[1] == 768, f\"Unexpected text_features shape: {text_features.shape}\"\n","\n","        # Concatenate image and text features\n","        combined_features = torch.cat((img_features, text_features), dim=1)\n","\n","        # Ensure combined_features shape is correct\n","        assert combined_features.shape[1] == 2432, f\"Unexpected combined_features shape: {combined_features.shape}\"\n","\n","        # Pass through the shared classifier\n","        logits = self.fc(combined_features)\n","        return logits\n","\n","# Move models to the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","densenet169.to(device)\n","bert_model.to(device)\n","\n","# Initialize the model\n","num_classes = 6  # Number of classes\n","model = EarlyFusionModel(densenet169, bert_model, num_classes)\n","model.to(device)\n","\n","# Define optimizer and loss function\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","num_epochs = 35\n","max_seq_length = 100  # Set your desired maximum sequence length\n","\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","\n","start_time = time.time()\n","\n","# Assuming train_loader and val_loader are already defined\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","        # Move tensors to the device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        logits = model(images, input_ids, attention_mask)\n","\n","        # Ensure labels have the correct shape and type\n","        labels = labels.view(-1)  # Flatten labels to match batch size\n","        labels = labels.to(torch.long)  # Ensure labels are of type torch.long\n","\n","        # Check if labels are empty\n","        if labels.numel() == 0:\n","            print(f\"Skipping empty labels batch\")\n","            continue\n","\n","        # Calculate loss\n","        loss = criterion(logits, labels)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, predicted = logits.max(1)\n","        total_train += labels.size(0)\n","        correct_train += predicted.eq(labels).sum().item()\n","\n","    epoch_train_loss = running_train_loss / len(train_loader)\n","    epoch_train_accuracy = correct_train / total_train\n","\n","    train_losses.append(epoch_train_loss)\n","    train_accuracies.append(epoch_train_accuracy)\n","\n","    # Validation loop\n","    model.eval()\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_texts, val_labels in val_loader:\n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","\n","            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n","            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n","            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n","\n","            # Forward pass\n","            val_logits = model(val_images, val_input_ids, val_attention_mask)\n","\n","            # Ensure val_labels have the correct shape and type\n","            val_labels = val_labels.view(-1)  # Flatten val_labels to match batch size\n","            val_labels = val_labels.to(torch.long)  # Ensure val_labels are of type torch.long\n","\n","            # Check if val_labels are empty\n","            if val_labels.numel() == 0:\n","                print(f\"Skipping empty validation labels batch\")\n","                continue\n","\n","            # Calculate validation loss\n","            val_loss = criterion(val_logits, val_labels)\n","\n","            running_val_loss += val_loss.item()\n","\n","            # Calculate validation accuracy\n","            _, val_predicted = val_logits.max(1)\n","            total_val += val_labels.size(0)\n","            correct_val += val_predicted.eq(val_labels).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_loader)\n","    epoch_val_accuracy = correct_val / total_val\n","\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_accuracy)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n","          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n","          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Total execution time: {execution_time:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:23:26.921188Z","iopub.status.busy":"2024-06-23T08:23:26.920700Z","iopub.status.idle":"2024-06-23T08:23:27.699628Z","shell.execute_reply":"2024-06-23T08:23:27.698720Z","shell.execute_reply.started":"2024-06-23T08:23:26.921154Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import numpy as np\n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","\n","# Evaluation function for the test dataset\n","def evaluate_model(model, test_loader, criterion):\n","    model.eval()\n","    running_test_loss = 0.0\n","    correct_test = 0\n","    total_test = 0\n","\n","    all_labels = []\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for test_images, test_texts, test_labels in test_loader:\n","            test_images = test_images.to(device)\n","            test_labels = test_labels.to(device)\n","\n","            test_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in test_texts]\n","            test_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in test_texts], dim=0).to(device)\n","            test_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in test_texts], dim=0).to(device)\n","\n","            # Forward pass\n","            test_logits = model(test_images, test_input_ids, test_attention_mask)\n","\n","            # Ensure test_labels have the correct shape and type\n","            test_labels = test_labels.view(-1)  # Flatten test_labels to match batch size\n","            test_labels = test_labels.to(torch.long)  # Ensure test_labels are of type torch.long\n","\n","            # Calculate test loss\n","            test_loss = criterion(test_logits, test_labels)\n","\n","            running_test_loss += test_loss.item()\n","\n","            # Calculate test accuracy\n","            _, test_predicted = test_logits.max(1)\n","            total_test += test_labels.size(0)\n","            correct_test += test_predicted.eq(test_labels).sum().item()\n","\n","            all_labels.extend(test_labels.cpu().numpy())\n","            all_predictions.extend(test_predicted.cpu().numpy())\n","\n","    epoch_test_loss = running_test_loss / len(test_loader)\n","    epoch_test_accuracy = correct_test / total_test\n","\n","    return epoch_test_loss, epoch_test_accuracy, all_labels, all_predictions\n","\n","# Run evaluation on the test dataset\n","test_loss, test_accuracy, test_labels, test_predictions = evaluate_model(model, test_loader, criterion)\n","\n","# Calculate metrics\n","accuracy = accuracy_score(test_labels, test_predictions)\n","precision = precision_score(test_labels, test_predictions, average='weighted')\n","recall = recall_score(test_labels, test_predictions, average='weighted')\n","f1 = f1_score(test_labels, test_predictions, average='weighted')\n","conf_matrix = confusion_matrix(test_labels, test_predictions)\n","\n","# Print metrics\n","print(f\"Test Loss: {test_loss:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Test Precision: {precision:.4f}\")\n","print(f\"Test Recall: {recall:.4f}\")\n","print(f\"Test F1 Score: {f1:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T08:23:58.418293Z","iopub.status.busy":"2024-06-23T08:23:58.417897Z","iopub.status.idle":"2024-06-23T08:23:58.830450Z","shell.execute_reply":"2024-06-23T08:23:58.829518Z","shell.execute_reply.started":"2024-06-23T08:23:58.418264Z"},"trusted":true},"outputs":[],"source":["# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","# Class names according to the label encoding mapping\n","class_names = ['Advocative', 'Controversial', 'ExhIbitionist', 'Expressive', 'Informative', 'Promotive']\n","sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5226942,"sourceId":8712717,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
