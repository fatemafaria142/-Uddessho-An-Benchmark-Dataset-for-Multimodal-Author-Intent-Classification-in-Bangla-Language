{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-23T05:00:34.754563Z","iopub.status.busy":"2024-06-23T05:00:34.754209Z","iopub.status.idle":"2024-06-23T05:00:41.435215Z","shell.execute_reply":"2024-06-23T05:00:41.434095Z","shell.execute_reply.started":"2024-06-23T05:00:34.754522Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertTokenizer, BertModel, AdamW\n","import torchvision.models as models\n","from torch import nn\n","import time\n","from tqdm import tqdm\n","\n","# Paths to the CSV files and image directories\n","csv_paths = {\n","    'train': '/kaggle/input/intent/Intent/train.csv',\n","    'test': '/kaggle/input/intent/Intent/validation.csv',\n","    'validation': '/kaggle/input/intent/Intent/validation.csv'\n","}\n","\n","image_dirs = {\n","    'train': '/kaggle/input/intent/Intent/train',\n","    'test': '/kaggle/input/intent/Intent/validation',\n","    'validation': '/kaggle/input/intent/Intent/validation'\n","}\n","output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n","\n","# Function to check for matching Meme_ID and image files, and add image paths\n","def check_matches(csv_path, image_dir):\n","    df = pd.read_csv(csv_path)\n","    image_files = os.listdir(image_dir)\n","    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n","    \n","    # Add Image_Path column to the dataframe\n","    df['Image_Path'] = df['Image_ID'].apply(lambda x: image_names.get(x, None))\n","    \n","    # Filter rows where Image_Path is not None (i.e., matched Meme_IDs)\n","    matched_df = df[df['Image_Path'].notna()]\n","    \n","    return matched_df\n","\n","# Function to encode Intent_Taxonomy classes into labels\n","def encode_labels(df):\n","    label_encoder = LabelEncoder()\n","    df['Intent_Taxonomy_Labels'] = label_encoder.fit_transform(df['Intent_Taxonomy'])\n","    return df, label_encoder.classes_\n","\n","# Check matches for each set (Train, Test, Validation)\n","for key in csv_paths:\n","    matched_df = check_matches(csv_paths[key], image_dirs[key])\n","    \n","    # Encode Intent_Taxonomy labels\n","    matched_df, classes = encode_labels(matched_df)\n","    \n","    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n","    \n","    # Save the processed dataframe to CSV\n","    matched_df.to_csv(matches_output_path, index=False)\n","    \n","    print(f\"{key} set:\")\n","    print(f\"Matched Meme_IDs with image paths and labels saved to {matches_output_path}\")\n","    print(f\"Classes and their corresponding labels:\\n{dict(zip(classes, range(len(classes))))}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:01:01.418646Z","iopub.status.busy":"2024-06-23T05:01:01.417951Z","iopub.status.idle":"2024-06-23T05:01:01.436929Z","shell.execute_reply":"2024-06-23T05:01:01.435998Z","shell.execute_reply.started":"2024-06-23T05:01:01.418614Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/train_matches.csv')\n","train_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:01:03.914417Z","iopub.status.busy":"2024-06-23T05:01:03.914071Z","iopub.status.idle":"2024-06-23T05:01:03.929315Z","shell.execute_reply":"2024-06-23T05:01:03.928310Z","shell.execute_reply.started":"2024-06-23T05:01:03.914390Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/working/test_matches.csv')\n","test_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:01:06.216378Z","iopub.status.busy":"2024-06-23T05:01:06.216057Z","iopub.status.idle":"2024-06-23T05:01:06.230969Z","shell.execute_reply":"2024-06-23T05:01:06.230054Z","shell.execute_reply.started":"2024-06-23T05:01:06.216355Z"},"trusted":true},"outputs":[],"source":["validation_df = pd.read_csv('/kaggle/working/validation_matches.csv')\n","validation_df.head(10)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:15:48.147272Z","iopub.status.busy":"2024-06-23T05:15:48.146454Z","iopub.status.idle":"2024-06-23T05:15:48.155685Z","shell.execute_reply":"2024-06-23T05:15:48.154789Z","shell.execute_reply.started":"2024-06-23T05:15:48.147231Z"},"trusted":true},"outputs":[],"source":["# Define your transformations using transforms.Compose\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Crop the center to 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","class MyMultimodalDataset(Dataset):\n","    def __init__(self, image_paths, image_captions, intent_taxonomy_labels, transform=None):\n","        self.image_paths = image_paths\n","        self.image_captions = image_captions\n","        self.intent_taxonomy = intent_taxonomy_labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        text = self.image_captions[idx]\n","        label = self.intent_taxonomy[idx]\n","\n","        # Load and preprocess image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, text, label"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:15:51.167109Z","iopub.status.busy":"2024-06-23T05:15:51.166389Z","iopub.status.idle":"2024-06-23T05:15:51.173561Z","shell.execute_reply":"2024-06-23T05:15:51.172624Z","shell.execute_reply.started":"2024-06-23T05:15:51.167077Z"},"trusted":true},"outputs":[],"source":["# Assuming you have lists or arrays of image paths, captions, and encoded labels:\n","train_dataset = MyMultimodalDataset(train_df['Image_Path'], train_df['Image_Caption'], train_df['Intent_Taxonomy_Labels'], transform=transform)\n","val_dataset = MyMultimodalDataset(validation_df['Image_Path'], validation_df['Image_Caption'], validation_df['Intent_Taxonomy_Labels'], transform=transform)\n","test_dataset = MyMultimodalDataset(test_df['Image_Path'], test_df['Image_Caption'], test_df['Intent_Taxonomy_Labels'], transform=transform)\n","\n","# Define data loaders\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:06:58.668320Z","iopub.status.busy":"2024-06-23T05:06:58.667960Z","iopub.status.idle":"2024-06-23T05:06:58.673196Z","shell.execute_reply":"2024-06-23T05:06:58.672337Z","shell.execute_reply.started":"2024-06-23T05:06:58.668293Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertModel, BertTokenizer,AdamW\n","from tqdm import tqdm\n","import torchvision.models as models\n","import time"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:15:53.364893Z","iopub.status.busy":"2024-06-23T05:15:53.364500Z","iopub.status.idle":"2024-06-23T05:15:54.839933Z","shell.execute_reply":"2024-06-23T05:15:54.838885Z","shell.execute_reply.started":"2024-06-23T05:15:53.364862Z"},"trusted":true},"outputs":[],"source":["# Initialize resnet50 with IMAGENET1K_V1 weights\n","resnet50 = models.resnet50(weights='IMAGENET1K_V1', progress=True)\n","resnet50 = torch.nn.Sequential(*(list(resnet50.children())[:-1]))  # Remove the classification layer\n","\n","# Initialize BERT tokenizer and model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:15:56.931653Z","iopub.status.busy":"2024-06-23T05:15:56.931274Z","iopub.status.idle":"2024-06-23T05:15:56.936416Z","shell.execute_reply":"2024-06-23T05:15:56.935542Z","shell.execute_reply.started":"2024-06-23T05:15:56.931622Z"},"trusted":true},"outputs":[],"source":["# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:15:59.276326Z","iopub.status.busy":"2024-06-23T05:15:59.275417Z","iopub.status.idle":"2024-06-23T05:15:59.322104Z","shell.execute_reply":"2024-06-23T05:15:59.321144Z","shell.execute_reply.started":"2024-06-23T05:15:59.276284Z"},"trusted":true},"outputs":[],"source":["resnet50.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:16:06.446266Z","iopub.status.busy":"2024-06-23T05:16:06.445319Z","iopub.status.idle":"2024-06-23T05:16:06.645961Z","shell.execute_reply":"2024-06-23T05:16:06.644876Z","shell.execute_reply.started":"2024-06-23T05:16:06.446217Z"},"trusted":true},"outputs":[],"source":["bert_model.to(device)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:16:38.234484Z","iopub.status.busy":"2024-06-23T05:16:38.233774Z","iopub.status.idle":"2024-06-23T05:16:38.239043Z","shell.execute_reply":"2024-06-23T05:16:38.238062Z","shell.execute_reply.started":"2024-06-23T05:16:38.234452Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","from torch.optim import AdamW\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:29:20.477461Z","iopub.status.busy":"2024-06-23T05:29:20.477089Z","iopub.status.idle":"2024-06-23T05:29:23.355129Z","shell.execute_reply":"2024-06-23T05:29:23.354197Z","shell.execute_reply.started":"2024-06-23T05:29:20.477433Z"},"trusted":true},"outputs":[],"source":["# Hyperparameters\n","num_epochs = 35\n","num_classes = 6  # Assuming you have 6 classes\n","max_seq_length = 100  # Set your desired maximum sequence length\n","learning_rate = 0.001\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = AdamW(list(resnet50.parameters()) + list(bert_model.parameters()), lr=learning_rate)\n","\n","# Training and validation lists for losses and accuracies\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","\n","start_time = time.time()\n","# Define the combined classifier for early fusion\n","class EarlyFusionClassifier(nn.Module):\n","    def __init__(self, img_feature_dim, text_feature_dim, num_classes):\n","        super(EarlyFusionClassifier, self).__init__()\n","        self.fc1 = nn.Linear(img_feature_dim + text_feature_dim, 512)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(512, num_classes)\n","\n","    def forward(self, img_features, text_features):\n","        combined_features = torch.cat((img_features, text_features), dim=1)\n","        x = self.fc1(combined_features)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        logits = self.fc2(x)\n","        return logits\n","\n","combined_classifier = EarlyFusionClassifier(img_feature_dim=2048, text_feature_dim=768, num_classes=num_classes).to(device)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    resnet50.train()\n","    bert_model.train()\n","    combined_classifier.train()\n","\n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","        # Move tensors to the device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Preprocess images before passing to resnet50\n","        preprocess = transforms.Compose([\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","        # Apply transformations to images if they are not already tensors\n","        if not torch.is_tensor(images):\n","            images = torch.stack([preprocess(image) for image in images])\n","\n","        optimizer.zero_grad()\n","\n","        img_feats = resnet50(images)\n","        img_feats = img_feats.view(img_feats.size(0), -1)  # Ensure img_feats has the shape (batch_size, 2048)\n","\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]  # Extract the [CLS] token representation\n","        text_feats = text_feats.view(text_feats.size(0), -1)  # Ensure text_feats has the shape (batch_size, 768)\n","\n","\n","        # Concatenate image and text features\n","        combined_feats = torch.cat((img_feats, text_feats), dim=1)\n","\n","\n","        # Get predictions from the combined classifier\n","        combined_logits = combined_classifier(img_feats, text_feats)\n","\n","        # Ensure labels have the correct shape and type\n","        labels = labels.view(-1)  # Flatten labels to match batch size\n","        labels = labels.to(torch.long)  # Ensure labels are of type torch.long\n","\n","        # Check if labels are empty\n","        if labels.numel() == 0:\n","            print(f\"Skipping empty labels batch\")\n","            continue\n","\n","        # Adjust the shape of combined_logits to match the batch size of labels\n","        combined_logits = combined_logits.view(labels.size(0), -1)\n","\n","        # Calculate loss\n","        loss = criterion(combined_logits, labels)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item()\n","        _, predicted = combined_logits.max(1)\n","        total_train += labels.size(0)\n","        correct_train += predicted.eq(labels).sum().item()\n","\n","    epoch_train_loss = running_train_loss / len(train_loader)\n","    epoch_train_accuracy = correct_train / total_train\n","\n","    train_losses.append(epoch_train_loss)\n","    train_accuracies.append(epoch_train_accuracy)\n","\n","    # Validation loop\n","    resnet50.eval()\n","    bert_model.eval()\n","    combined_classifier.eval()\n","\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_texts, val_labels in val_loader:\n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","\n","            if not torch.is_tensor(val_images):\n","                val_images = torch.stack([preprocess(image) for image in val_images])\n","\n","            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n","            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n","            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n","\n","            val_img_feats = resnet50(val_images)\n","            val_img_feats = val_img_feats.view(val_img_feats.size(0), -1)  # Ensure val_img_feats has the shape (batch_size, 2048)\n","\n","            val_outputs = bert_model(val_input_ids, attention_mask=val_attention_mask)\n","            val_text_feats = val_outputs.last_hidden_state[:, 0, :]  # Extract the [CLS] token representation\n","            val_text_feats = val_text_feats.view(val_text_feats.size(0), -1)  # Ensure val_text_feats has the shape (batch_size, 768)\n","\n","            # Concatenate image and text features\n","            combined_feats = torch.cat((val_img_feats, val_text_feats), dim=1)\n","\n","\n","            # Get predictions from the combined classifier\n","            val_combined_logits = combined_classifier(val_img_feats, val_text_feats)\n","\n","            # Ensure the val_labels tensor is flattened to match the val_combined_logits batch size\n","            val_labels = val_labels.view(-1)\n","\n","            val_loss = criterion(val_combined_logits, val_labels)\n","\n","            running_val_loss += val_loss.item()\n","            _, val_predicted = val_combined_logits.max(1)\n","            total_val += val_labels.size(0)\n","            correct_val += val_predicted.eq(val_labels).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_loader)\n","    epoch_val_accuracy = correct_val / total_val\n","\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_accuracy)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n","          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n","          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Total execution time: {execution_time:.2f} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:29:30.247032Z","iopub.status.busy":"2024-06-23T05:29:30.246666Z","iopub.status.idle":"2024-06-23T05:29:30.816003Z","shell.execute_reply":"2024-06-23T05:29:30.814947Z","shell.execute_reply.started":"2024-06-23T05:29:30.247004Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","from tqdm import tqdm\n","\n","# Assuming test_loader contains your test data loader\n","\n","combined_classifier.eval()  # Set the model to evaluation mode\n","resnet50.eval()  # Set the ResNet model to evaluation mode\n","bert_model.eval()  # Set the BERT model to evaluation mode\n","\n","test_losses = []\n","predictions = []\n","true_labels = []\n","\n","with torch.no_grad():\n","    for images, texts, labels in tqdm(test_loader, desc='Testing', leave=False):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Tokenize and prepare text inputs\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        # Extract features using ResNet and BERT\n","        img_feats = resnet50(images)\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        # Get combined logits from the classifier\n","        logits = combined_classifier(img_feats, text_feats)\n","\n","        # Calculate loss\n","        test_loss = criterion(logits, labels)\n","        test_losses.append(test_loss.item())\n","\n","        # Get predictions\n","        _, predicted = logits.max(1)\n","        predictions.extend(predicted.cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","\n","# Calculate overall test metrics\n","average_test_loss = sum(test_losses) / len(test_losses)\n","test_accuracy = accuracy_score(true_labels, predictions)\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_matrix = confusion_matrix(true_labels, predictions)\n","\n","# Print results\n","print(f\"Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:29:33.979298Z","iopub.status.busy":"2024-06-23T05:29:33.978705Z","iopub.status.idle":"2024-06-23T05:29:34.386111Z","shell.execute_reply":"2024-06-23T05:29:34.385189Z","shell.execute_reply.started":"2024-06-23T05:29:33.979270Z"},"trusted":true},"outputs":[],"source":["# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","# Class names according to the label encoding mapping\n","class_names = ['Advocative', 'Controversial', 'ExhIbitionist', 'Expressive', 'Informative', 'Promotive']\n","sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5226942,"sourceId":8712717,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
