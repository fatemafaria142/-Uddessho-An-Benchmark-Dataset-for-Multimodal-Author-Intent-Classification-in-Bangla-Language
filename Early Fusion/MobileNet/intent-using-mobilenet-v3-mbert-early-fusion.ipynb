{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-23T05:00:34.754563Z","iopub.status.busy":"2024-06-23T05:00:34.754209Z","iopub.status.idle":"2024-06-23T05:00:41.435215Z","shell.execute_reply":"2024-06-23T05:00:41.434095Z","shell.execute_reply.started":"2024-06-23T05:00:34.754522Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertTokenizer, BertModel, AdamW\n","import torchvision.models as models\n","from torch import nn\n","import time\n","from tqdm import tqdm\n","\n","# Paths to the CSV files and image directories\n","csv_paths = {\n","    'train': '/kaggle/input/intent/Intent/train.csv',\n","    'test': '/kaggle/input/intent/Intent/validation.csv',\n","    'validation': '/kaggle/input/intent/Intent/validation.csv'\n","}\n","\n","image_dirs = {\n","    'train': '/kaggle/input/intent/Intent/train',\n","    'test': '/kaggle/input/intent/Intent/validation',\n","    'validation': '/kaggle/input/intent/Intent/validation'\n","}\n","output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n","\n","# Function to check for matching Meme_ID and image files, and add image paths\n","def check_matches(csv_path, image_dir):\n","    df = pd.read_csv(csv_path)\n","    image_files = os.listdir(image_dir)\n","    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n","    \n","    # Add Image_Path column to the dataframe\n","    df['Image_Path'] = df['Image_ID'].apply(lambda x: image_names.get(x, None))\n","    \n","    # Filter rows where Image_Path is not None (i.e., matched Meme_IDs)\n","    matched_df = df[df['Image_Path'].notna()]\n","    \n","    return matched_df\n","\n","# Function to encode Intent_Taxonomy classes into labels\n","def encode_labels(df):\n","    label_encoder = LabelEncoder()\n","    df['Intent_Taxonomy_Labels'] = label_encoder.fit_transform(df['Intent_Taxonomy'])\n","    return df, label_encoder.classes_\n","\n","# Check matches for each set (Train, Test, Validation)\n","for key in csv_paths:\n","    matched_df = check_matches(csv_paths[key], image_dirs[key])\n","    \n","    # Encode Intent_Taxonomy labels\n","    matched_df, classes = encode_labels(matched_df)\n","    \n","    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n","    \n","    # Save the processed dataframe to CSV\n","    matched_df.to_csv(matches_output_path, index=False)\n","    \n","    print(f\"{key} set:\")\n","    print(f\"Matched Meme_IDs with image paths and labels saved to {matches_output_path}\")\n","    print(f\"Classes and their corresponding labels:\\n{dict(zip(classes, range(len(classes))))}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:01:01.418646Z","iopub.status.busy":"2024-06-23T05:01:01.417951Z","iopub.status.idle":"2024-06-23T05:01:01.436929Z","shell.execute_reply":"2024-06-23T05:01:01.435998Z","shell.execute_reply.started":"2024-06-23T05:01:01.418614Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/train_matches.csv')\n","train_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:01:03.914417Z","iopub.status.busy":"2024-06-23T05:01:03.914071Z","iopub.status.idle":"2024-06-23T05:01:03.929315Z","shell.execute_reply":"2024-06-23T05:01:03.928310Z","shell.execute_reply.started":"2024-06-23T05:01:03.914390Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/working/test_matches.csv')\n","test_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:01:06.216378Z","iopub.status.busy":"2024-06-23T05:01:06.216057Z","iopub.status.idle":"2024-06-23T05:01:06.230969Z","shell.execute_reply":"2024-06-23T05:01:06.230054Z","shell.execute_reply.started":"2024-06-23T05:01:06.216355Z"},"trusted":true},"outputs":[],"source":["validation_df = pd.read_csv('/kaggle/working/validation_matches.csv')\n","validation_df.head(10)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:15:48.147272Z","iopub.status.busy":"2024-06-23T05:15:48.146454Z","iopub.status.idle":"2024-06-23T05:15:48.155685Z","shell.execute_reply":"2024-06-23T05:15:48.154789Z","shell.execute_reply.started":"2024-06-23T05:15:48.147231Z"},"trusted":true},"outputs":[],"source":["# Define your transformations using transforms.Compose\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Crop the center to 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","class MyMultimodalDataset(Dataset):\n","    def __init__(self, image_paths, image_captions, intent_taxonomy_labels, transform=None):\n","        self.image_paths = image_paths\n","        self.image_captions = image_captions\n","        self.intent_taxonomy = intent_taxonomy_labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        text = self.image_captions[idx]\n","        label = self.intent_taxonomy[idx]\n","\n","        # Load and preprocess image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, text, label"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T05:15:51.167109Z","iopub.status.busy":"2024-06-23T05:15:51.166389Z","iopub.status.idle":"2024-06-23T05:15:51.173561Z","shell.execute_reply":"2024-06-23T05:15:51.172624Z","shell.execute_reply.started":"2024-06-23T05:15:51.167077Z"},"trusted":true},"outputs":[],"source":["# Assuming you have lists or arrays of image paths, captions, and encoded labels:\n","train_dataset = MyMultimodalDataset(train_df['Image_Path'], train_df['Image_Caption'], train_df['Intent_Taxonomy_Labels'], transform=transform)\n","val_dataset = MyMultimodalDataset(validation_df['Image_Path'], validation_df['Image_Caption'], validation_df['Intent_Taxonomy_Labels'], transform=transform)\n","test_dataset = MyMultimodalDataset(test_df['Image_Path'], test_df['Image_Caption'], test_df['Intent_Taxonomy_Labels'], transform=transform)\n","\n","# Define data loaders\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T06:10:04.918559Z","iopub.status.busy":"2024-06-23T06:10:04.917912Z","iopub.status.idle":"2024-06-23T06:10:04.923429Z","shell.execute_reply":"2024-06-23T06:10:04.922474Z","shell.execute_reply.started":"2024-06-23T06:10:04.918516Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertModel, BertTokenizer,AdamW\n","from tqdm import tqdm\n","import torchvision.models as models\n","import time\n","from torchvision.models import mobilenet_v3_small"]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T06:13:20.634134Z","iopub.status.busy":"2024-06-23T06:13:20.633290Z","iopub.status.idle":"2024-06-23T06:13:21.548227Z","shell.execute_reply":"2024-06-23T06:13:21.547448Z","shell.execute_reply.started":"2024-06-23T06:13:20.634101Z"},"trusted":true},"outputs":[],"source":["# Initialize mobilenet_v3_small with IMAGENET1K_V1 weights\n","mobilenet_v3_small = models.mobilenet_v3_small(weights='IMAGENET1K_V1', progress=True)\n","mobilenet_v3_small = torch.nn.Sequential(*(list(mobilenet_v3_small.children())[:-1]))  # Remove the classification layer\n","mobilenet_v3_model.to(device)\n","\n","# Initialize BERT tokenizer and model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T06:13:25.191645Z","iopub.status.busy":"2024-06-23T06:13:25.190922Z","iopub.status.idle":"2024-06-23T06:13:25.196692Z","shell.execute_reply":"2024-06-23T06:13:25.195806Z","shell.execute_reply.started":"2024-06-23T06:13:25.191615Z"},"trusted":true},"outputs":[],"source":["# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T06:13:35.108268Z","iopub.status.busy":"2024-06-23T06:13:35.107620Z","iopub.status.idle":"2024-06-23T06:13:35.127216Z","shell.execute_reply":"2024-06-23T06:13:35.126361Z","shell.execute_reply.started":"2024-06-23T06:13:35.108239Z"},"trusted":true},"outputs":[],"source":["mobilenet_v3_small.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T06:13:40.988453Z","iopub.status.busy":"2024-06-23T06:13:40.988069Z","iopub.status.idle":"2024-06-23T06:13:41.182066Z","shell.execute_reply":"2024-06-23T06:13:41.181218Z","shell.execute_reply.started":"2024-06-23T06:13:40.988422Z"},"trusted":true},"outputs":[],"source":["bert_model.to(device)"]},{"cell_type":"code","execution_count":126,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T06:18:58.090095Z","iopub.status.busy":"2024-06-23T06:18:58.089726Z","iopub.status.idle":"2024-06-23T06:18:58.094933Z","shell.execute_reply":"2024-06-23T06:18:58.094075Z","shell.execute_reply.started":"2024-06-23T06:18:58.090067Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","from torch.optim import AdamW\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T06:31:24.544274Z","iopub.status.busy":"2024-06-23T06:31:24.543908Z","iopub.status.idle":"2024-06-23T06:31:26.141406Z","shell.execute_reply":"2024-06-23T06:31:26.140474Z","shell.execute_reply.started":"2024-06-23T06:31:24.544246Z"},"trusted":true},"outputs":[],"source":["# Set models to evaluation mode\n","mobilenet_v3_small.eval()\n","bert_model.eval()\n","\n","num_epochs = 35\n","num_classes = 6\n","max_seq_length = 100  # Set your desired maximum sequence length\n","\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","\n","start_time = time.time()\n","\n","# Define the Early Fusion Classifier\n","class EarlyFusionClassifier(nn.Module):\n","    def __init__(self, img_feature_dim, text_feature_dim, num_classes):\n","        super(EarlyFusionClassifier, self).__init__()\n","        self.fc1 = nn.Linear(img_feature_dim + text_feature_dim, 512)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(512, num_classes)\n","\n","    def forward(self, img_features, text_features):\n","        combined_features = torch.cat((img_features, text_features), dim=1)\n","        x = self.fc1(combined_features)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# Initialize Early Fusion classifier with corrected input dimensions\n","early_fusion_classifier = EarlyFusionClassifier(img_feature_dim=576, text_feature_dim=768, num_classes=6).to(device)\n","\n","# Define optimizer and criterion\n","optimizer = AdamW(early_fusion_classifier.parameters(), lr=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","        # Move tensors to the device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Extract features from images and texts\n","        img_feats = mobilenet_v3_small(images)\n","        img_feats = img_feats.squeeze()\n","        text_outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = text_outputs.last_hidden_state[:, 0, :]\n","\n","        # Ensure features are 2-dimensional\n","        if img_feats.dim() == 1:\n","            img_feats = img_feats.unsqueeze(0)\n","        if text_feats.dim() == 1:\n","            text_feats = text_feats.unsqueeze(0)\n","\n","\n","        # Forward pass through Early Fusion classifier\n","        combined_logits = early_fusion_classifier(img_feats, text_feats)\n","\n","        # Ensure labels have the correct shape and type\n","        labels = labels.view(-1)  # Flatten labels to match batch size\n","        labels = labels.to(torch.long)  # Ensure labels are of type torch.long\n","\n","        # Calculate loss\n","        loss = criterion(combined_logits, labels)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, predicted = combined_logits.max(1)\n","        total_train += labels.size(0)\n","        correct_train += predicted.eq(labels).sum().item()\n","\n","    epoch_train_loss = running_train_loss / len(train_loader)\n","    epoch_train_accuracy = correct_train / total_train\n","\n","    train_losses.append(epoch_train_loss)\n","    train_accuracies.append(epoch_train_accuracy)\n","\n","    # Validation loop\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_texts, val_labels in val_loader:\n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","\n","            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n","            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n","            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n","\n","            # Extract features from images and texts for validation\n","            val_img_feats = mobilenet_v3_small(val_images)\n","            val_img_feats = val_img_feats.squeeze()\n","            val_text_outputs = bert_model(val_input_ids, attention_mask=val_attention_mask)\n","            val_text_feats = val_text_outputs.last_hidden_state[:, 0, :]\n","\n","            # Ensure features are 2-dimensional\n","            if val_img_feats.dim() == 1:\n","                val_img_feats = val_img_feats.unsqueeze(0)\n","            if val_text_feats.dim() == 1:\n","                val_text_feats = val_text_feats.unsqueeze(0)\n","\n","            # Forward pass through Early Fusion classifier for validation\n","            val_combined_logits = early_fusion_classifier(val_img_feats, val_text_feats)\n","\n","            # Ensure val_labels have the correct shape and type\n","            val_labels = val_labels.view(-1)  # Flatten val_labels to match batch size\n","            val_labels = val_labels.to(torch.long)  # Ensure val_labels are of type torch.long\n","\n","            # Calculate validation loss\n","            val_loss = criterion(val_combined_logits, val_labels)\n","\n","            running_val_loss += val_loss.item()\n","\n","            # Calculate validation accuracy\n","            _, val_predicted = val_combined_logits.max(1)\n","            total_val += val_labels.size(0)\n","            correct_val += val_predicted.eq(val_labels).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_loader)\n","    epoch_val_accuracy = correct_val / total_val\n","\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_accuracy)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n","          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n","          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Total execution time: {execution_time:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T06:35:19.697251Z","iopub.status.busy":"2024-06-23T06:35:19.696969Z","iopub.status.idle":"2024-06-23T06:35:20.266840Z","shell.execute_reply":"2024-06-23T06:35:20.265977Z","shell.execute_reply.started":"2024-06-23T06:35:19.697227Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","from tqdm import tqdm\n","\n","# Assuming test_loader contains your test data loader\n","\n","early_fusion_classifier.eval()  # Set the model to evaluation mode\n","\n","test_losses = []\n","test_accuracies = []\n","predictions = []\n","true_labels = []\n","\n","with torch.no_grad():\n","    for images, texts, labels in tqdm(test_loader, desc='Testing', leave=False):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        img_feats = mobilenet_v3_small(images)\n","        img_feats = img_feats.view(img_feats.size(0), -1)  # Flatten the image features\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]  # Extract the [CLS] token representation\n","\n","        if img_feats.dim() == 1:\n","            img_feats = img_feats.unsqueeze(0)\n","        if text_feats.dim() == 1:\n","            text_feats = text_feats.unsqueeze(0)\n","\n","        logits = early_fusion_classifier(img_feats, text_feats)\n","\n","        # Calculate loss if needed\n","        test_loss = criterion(logits, labels)\n","        test_losses.append(test_loss.item())\n","\n","        # Calculate accuracy\n","        _, predicted = logits.max(1)\n","        predictions.extend(predicted.cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","        correct = predicted.eq(labels).sum().item()\n","        total = labels.size(0)\n","        test_accuracy = correct / total\n","        test_accuracies.append(test_accuracy)\n","\n","# Calculate overall test metrics\n","average_test_loss = sum(test_losses) / len(test_losses)\n","test_accuracy = accuracy_score(true_labels, predictions)\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_matrix = confusion_matrix(true_labels, predictions)\n","\n","# Print and plot results\n","print(f\"Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T06:35:26.005904Z","iopub.status.busy":"2024-06-23T06:35:26.005013Z","iopub.status.idle":"2024-06-23T06:35:26.388360Z","shell.execute_reply":"2024-06-23T06:35:26.387487Z","shell.execute_reply.started":"2024-06-23T06:35:26.005872Z"},"trusted":true},"outputs":[],"source":["# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","# Class names according to the label encoding mapping\n","class_names = ['Advocative', 'Controversial', 'ExhIbitionist', 'Expressive', 'Informative', 'Promotive']\n","sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5226942,"sourceId":8712717,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
