{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-18T04:24:41.259320Z","iopub.status.busy":"2024-06-18T04:24:41.258600Z","iopub.status.idle":"2024-06-18T04:24:41.296416Z","shell.execute_reply":"2024-06-18T04:24:41.295331Z","shell.execute_reply.started":"2024-06-18T04:24:41.259285Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertTokenizer, BertModel, AdamW\n","import torchvision.models as models\n","from torch import nn\n","import time\n","from tqdm import tqdm\n","\n","# Paths to the CSV files and image directories\n","csv_paths = {\n","    'train': '/kaggle/input/intent/Intent/train.csv',\n","    'test': '/kaggle/input/intent/Intent/validation.csv',\n","    'validation': '/kaggle/input/intent/Intent/validation.csv'\n","}\n","\n","image_dirs = {\n","    'train': '/kaggle/input/intent/Intent/train',\n","    'test': '/kaggle/input/intent/Intent/validation',\n","    'validation': '/kaggle/input/intent/Intent/validation'\n","}\n","output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n","\n","# Function to check for matching Meme_ID and image files, and add image paths\n","def check_matches(csv_path, image_dir):\n","    df = pd.read_csv(csv_path)\n","    image_files = os.listdir(image_dir)\n","    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n","    \n","    # Add Image_Path column to the dataframe\n","    df['Image_Path'] = df['Image_ID'].apply(lambda x: image_names.get(x, None))\n","    \n","    # Filter rows where Image_Path is not None (i.e., matched Meme_IDs)\n","    matched_df = df[df['Image_Path'].notna()]\n","    \n","    return matched_df\n","\n","# Function to encode Intent_Taxonomy classes into labels\n","def encode_labels(df):\n","    label_encoder = LabelEncoder()\n","    df['Intent_Taxonomy_Labels'] = label_encoder.fit_transform(df['Intent_Taxonomy'])\n","    return df, label_encoder.classes_\n","\n","# Check matches for each set (Train, Test, Validation)\n","for key in csv_paths:\n","    matched_df = check_matches(csv_paths[key], image_dirs[key])\n","    \n","    # Encode Intent_Taxonomy labels\n","    matched_df, classes = encode_labels(matched_df)\n","    \n","    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n","    \n","    # Save the processed dataframe to CSV\n","    matched_df.to_csv(matches_output_path, index=False)\n","    \n","    print(f\"{key} set:\")\n","    print(f\"Matched Meme_IDs with image paths and labels saved to {matches_output_path}\")\n","    print(f\"Classes and their corresponding labels:\\n{dict(zip(classes, range(len(classes))))}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T04:41:43.752808Z","iopub.status.busy":"2024-06-18T04:41:43.752423Z","iopub.status.idle":"2024-06-18T04:41:43.770022Z","shell.execute_reply":"2024-06-18T04:41:43.768786Z","shell.execute_reply.started":"2024-06-18T04:41:43.752780Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/train_matches.csv')\n","train_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:29:17.690621Z","iopub.status.busy":"2024-06-18T05:29:17.690239Z","iopub.status.idle":"2024-06-18T05:29:17.706251Z","shell.execute_reply":"2024-06-18T05:29:17.705240Z","shell.execute_reply.started":"2024-06-18T05:29:17.690590Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/working/test_matches.csv')\n","test_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T04:41:52.032256Z","iopub.status.busy":"2024-06-18T04:41:52.031414Z","iopub.status.idle":"2024-06-18T04:41:52.048452Z","shell.execute_reply":"2024-06-18T04:41:52.047575Z","shell.execute_reply.started":"2024-06-18T04:41:52.032226Z"},"trusted":true},"outputs":[],"source":["validation_df = pd.read_csv('/kaggle/working/validation_matches.csv')\n","validation_df.head(10)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T04:24:53.122433Z","iopub.status.busy":"2024-06-18T04:24:53.121626Z","iopub.status.idle":"2024-06-18T04:24:53.130531Z","shell.execute_reply":"2024-06-18T04:24:53.129598Z","shell.execute_reply.started":"2024-06-18T04:24:53.122400Z"},"trusted":true},"outputs":[],"source":["# Define your transformations using transforms.Compose\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Crop the center to 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","class MyMultimodalDataset(Dataset):\n","    def __init__(self, image_paths, image_captions, intent_taxonomy_labels, transform=None):\n","        self.image_paths = image_paths\n","        self.image_captions = image_captions\n","        self.intent_taxonomy = intent_taxonomy_labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        text = self.image_captions[idx]\n","        label = self.intent_taxonomy[idx]\n","\n","        # Load and preprocess image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, text, label"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T04:36:46.077406Z","iopub.status.busy":"2024-06-18T04:36:46.077036Z","iopub.status.idle":"2024-06-18T04:36:46.085507Z","shell.execute_reply":"2024-06-18T04:36:46.084076Z","shell.execute_reply.started":"2024-06-18T04:36:46.077358Z"},"trusted":true},"outputs":[],"source":["# Assuming you have lists or arrays of image paths, captions, and encoded labels:\n","train_dataset = MyMultimodalDataset(train_df['Image_Path'], train_df['Image_Caption'], train_df['Intent_Taxonomy_Labels'], transform=transform)\n","val_dataset = MyMultimodalDataset(validation_df['Image_Path'], validation_df['Image_Caption'], validation_df['Intent_Taxonomy_Labels'], transform=transform)\n","test_dataset = MyMultimodalDataset(test_df['Image_Path'], test_df['Image_Caption'], test_df['Intent_Taxonomy_Labels'], transform=transform)\n","\n","# Define data loaders\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":112,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:35:07.080484Z","iopub.status.busy":"2024-06-18T05:35:07.079801Z","iopub.status.idle":"2024-06-18T05:35:07.323741Z","shell.execute_reply":"2024-06-18T05:35:07.322909Z","shell.execute_reply.started":"2024-06-18T05:35:07.080451Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import densenet121  # Import inception_v3\n","from transformers import BertModel, BertTokenizer\n","from tqdm import tqdm\n","import torchvision.models as models\n","import time\n","\n","# Initialize densenet121_model with IMAGENET1K_V1 weights\n","densenet121 = models.densenet121(weights='IMAGENET1K_V1', progress=True)\n","densenet121 = torch.nn.Sequential(*(list(densenet121.children())[:-1]))  # Remove the classification layer"]},{"cell_type":"code","execution_count":114,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:35:11.368055Z","iopub.status.busy":"2024-06-18T05:35:11.367001Z","iopub.status.idle":"2024-06-18T05:35:12.188574Z","shell.execute_reply":"2024-06-18T05:35:12.187734Z","shell.execute_reply.started":"2024-06-18T05:35:11.367998Z"},"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer, BertModel,AdamW\n","# Initialize BERT tokenizer and model\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:35:14.587435Z","iopub.status.busy":"2024-06-18T05:35:14.586861Z","iopub.status.idle":"2024-06-18T05:35:14.592340Z","shell.execute_reply":"2024-06-18T05:35:14.591429Z","shell.execute_reply.started":"2024-06-18T05:35:14.587402Z"},"trusted":true},"outputs":[],"source":["# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:35:19.498559Z","iopub.status.busy":"2024-06-18T05:35:19.497857Z","iopub.status.idle":"2024-06-18T05:35:19.543122Z","shell.execute_reply":"2024-06-18T05:35:19.542161Z","shell.execute_reply.started":"2024-06-18T05:35:19.498519Z"},"trusted":true},"outputs":[],"source":["densenet121.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:35:26.787864Z","iopub.status.busy":"2024-06-18T05:35:26.786999Z","iopub.status.idle":"2024-06-18T05:35:26.989456Z","shell.execute_reply":"2024-06-18T05:35:26.988436Z","shell.execute_reply.started":"2024-06-18T05:35:26.787829Z"},"trusted":true},"outputs":[],"source":["bert_model.to(device)"]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:35:29.715572Z","iopub.status.busy":"2024-06-18T05:35:29.714847Z","iopub.status.idle":"2024-06-18T05:35:29.719934Z","shell.execute_reply":"2024-06-18T05:35:29.719113Z","shell.execute_reply.started":"2024-06-18T05:35:29.715539Z"},"trusted":true},"outputs":[],"source":["import torch\n","import time\n","from torch.optim import AdamW\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":119,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:35:34.970559Z","iopub.status.busy":"2024-06-18T05:35:34.970203Z","iopub.status.idle":"2024-06-18T05:35:34.983488Z","shell.execute_reply":"2024-06-18T05:35:34.982593Z","shell.execute_reply.started":"2024-06-18T05:35:34.970528Z"},"trusted":true},"outputs":[],"source":["# Define optimizer and loss function\n","optimizer = AdamW(list(densenet121.parameters()) + list(bert_model.parameters()), lr=2e-5)\n","criterion = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:42:56.999765Z","iopub.status.busy":"2024-06-18T05:42:56.999423Z","iopub.status.idle":"2024-06-18T05:43:00.476991Z","shell.execute_reply":"2024-06-18T05:43:00.476019Z","shell.execute_reply.started":"2024-06-18T05:42:56.999738Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from tqdm import tqdm\n","import time\n","\n","# Set models to evaluation mode\n","densenet121.eval()\n","bert_model.eval()\n","\n","num_epochs = 1\n","num_classes = 6\n","max_seq_length = 100  # Set your desired maximum sequence length\n","\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","\n","start_time = time.time()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","        # Move tensors to the device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        img_feats = densenet121(images)\n","        img_feats = img_feats.squeeze()\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        img_feats_reshaped = img_feats.view(img_feats.size(0), -1)  # Reshape img_feats\n","\n","        # Separate classifiers for image and text features\n","        img_classifier = torch.nn.Sequential(\n","            torch.nn.Linear(img_feats_reshaped.shape[1], 512).to(device),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(0.5),\n","            torch.nn.Linear(512, num_classes).to(device),\n","        )\n","\n","        text_classifier = torch.nn.Sequential(\n","            torch.nn.Linear(text_feats.shape[1], 512).to(device),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(0.5),\n","            torch.nn.Linear(512, num_classes).to(device),\n","        )\n","\n","        # Get predictions for image and text modalities separately\n","        img_logits = img_classifier(img_feats_reshaped)\n","        text_logits = text_classifier(text_feats)\n","\n","        # Combine predictions using a fusion technique (e.g., simple averaging)\n","        combined_logits = 0.5 * (img_logits + text_logits)  # Simple averaging\n","\n","        # Ensure labels have the correct shape and type\n","        labels = labels.view(-1)  # Flatten labels to match batch size\n","        labels = labels.to(torch.long)  # Ensure labels are of type torch.long\n","\n","        # Check if labels are empty\n","        if labels.numel() == 0:\n","            print(f\"Skipping empty labels batch\")\n","            continue\n","\n","        # Adjust the shape of combined_logits to match the batch size of labels\n","        combined_logits = combined_logits.view(labels.size(0), -1)\n","\n","        # Calculate loss\n","        loss = criterion(combined_logits, labels)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, predicted = combined_logits.max(1)\n","        total_train += labels.size(0)\n","        correct_train += predicted.eq(labels).sum().item()\n","\n","    epoch_train_loss = running_train_loss / len(train_loader)\n","    epoch_train_accuracy = correct_train / total_train\n","\n","    train_losses.append(epoch_train_loss)\n","    train_accuracies.append(epoch_train_accuracy)\n","\n","    # Validation loop\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_texts, val_labels in val_loader:\n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","\n","            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n","            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n","            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n","\n","            val_img_feats = densenet121(val_images)\n","            val_img_feats = val_img_feats.squeeze()\n","\n","            val_outputs = bert_model(val_input_ids, attention_mask=val_attention_mask)\n","            val_text_feats = val_outputs.last_hidden_state[:, 0, :]\n","\n","            val_img_feats_reshaped = val_img_feats.view(val_img_feats.size(0), -1)  # Reshape val_img_feats\n","\n","            # Separate classifiers for image and text features\n","            val_img_classifier = torch.nn.Sequential(\n","                torch.nn.Linear(val_img_feats_reshaped.shape[1], 512).to(device),\n","                torch.nn.ReLU(),\n","                torch.nn.Dropout(0.5),\n","                torch.nn.Linear(512, num_classes).to(device),\n","            )\n","\n","            val_text_classifier = torch.nn.Sequential(\n","                torch.nn.Linear(val_text_feats.shape[1], 512).to(device),\n","                torch.nn.ReLU(),\n","                torch.nn.Dropout(0.5),\n","                torch.nn.Linear(512, num_classes).to(device),\n","            )\n","\n","            # Get predictions for image and text modalities separately\n","            val_img_logits = val_img_classifier(val_img_feats_reshaped)\n","            val_text_logits = val_text_classifier(val_text_feats)\n","\n","            # Combine predictions using a fusion technique (e.g., simple averaging)\n","            val_combined_logits = 0.5 * (val_img_logits + val_text_logits)  # Simple averaging\n","\n","            # Ensure val_labels have the correct shape and type\n","            val_labels = val_labels.view(-1)  # Flatten val_labels to match batch size\n","            val_labels = val_labels.to(torch.long)  # Ensure val_labels are of type torch.long\n","\n","\n","            # Check if val_labels are empty\n","            if val_labels.numel() == 0:\n","                print(f\"Skipping empty validation labels batch\")\n","                continue\n","\n","            # Adjust the shape of val_combined_logits to match the batch size of val_labels\n","            val_combined_logits = val_combined_logits.view(val_labels.size(0), -1)\n","\n","            # Calculate validation loss\n","            val_loss = criterion(val_combined_logits, val_labels)\n","\n","            running_val_loss += val_loss.item()\n","\n","            # Calculate validation accuracy\n","            _, val_predicted = val_combined_logits.max(1)\n","            total_val += val_labels.size(0)\n","            correct_val += val_predicted.eq(val_labels).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_loader)\n","    epoch_val_accuracy = correct_val / total_val\n","\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_accuracy)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n","          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n","          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Total execution time: {execution_time:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:43:05.891197Z","iopub.status.busy":"2024-06-18T05:43:05.890838Z","iopub.status.idle":"2024-06-18T05:43:06.666134Z","shell.execute_reply":"2024-06-18T05:43:06.665084Z","shell.execute_reply.started":"2024-06-18T05:43:05.891167Z"},"trusted":true},"outputs":[],"source":["test_losses = []\n","test_accuracies = []\n","predicted_labels = []\n","true_labels = []\n","\n","# Test loop\n","with torch.no_grad():\n","    for test_images, test_texts, test_labels in test_loader:\n","        test_images = test_images.to(device)\n","        test_labels = test_labels.to(device)\n","\n","        test_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in test_texts]\n","        test_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in test_texts], dim=0).to(device)\n","        test_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in test_texts], dim=0).to(device)\n","\n","        test_img_feats = densenet121(test_images)\n","        test_img_feats = test_img_feats.squeeze()\n","\n","        test_outputs = bert_model(test_input_ids, attention_mask=test_attention_mask)\n","        test_text_feats = test_outputs.last_hidden_state[:, 0, :]\n","\n","        test_img_feats_reshaped = test_img_feats.view(test_img_feats.size(0), -1)  # Reshape test_img_feats\n","\n","        # Separate classifiers for image and text features\n","        test_img_classifier = torch.nn.Sequential(\n","            torch.nn.Linear(test_img_feats_reshaped.shape[1], 512).to(device),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(0.5),\n","            torch.nn.Linear(512, num_classes).to(device),\n","        )\n","\n","        test_text_classifier = torch.nn.Sequential(\n","            torch.nn.Linear(test_text_feats.shape[1], 512).to(device),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(0.5),\n","            torch.nn.Linear(512, num_classes).to(device),\n","        )\n","\n","        # Get predictions for image and text modalities separately\n","        test_img_logits = test_img_classifier(test_img_feats_reshaped)\n","        test_text_logits = test_text_classifier(test_text_feats)\n","\n","        # Combine predictions using a fusion technique (e.g., simple averaging)\n","        test_combined_logits = 0.5 * (test_img_logits + test_text_logits)  # Simple averaging\n","\n","        # Ensure test_labels have the correct shape and type\n","        test_labels = test_labels.view(-1)  # Flatten test_labels to match batch size\n","        test_labels = test_labels.to(torch.long)  # Ensure test_labels are of type torch.long\n","\n","        # Check if test_labels are empty\n","        if test_labels.numel() == 0:\n","            print(f\"Skipping empty test labels batch\")\n","            continue\n","\n","        # Adjust the shape of test_combined_logits to match the batch size of test_labels\n","        test_combined_logits = test_combined_logits.view(test_labels.size(0), -1)\n","\n","        # Calculate test loss\n","        test_loss = criterion(test_combined_logits, test_labels)\n","        test_losses.append(test_loss.item())\n","\n","        # Calculate test accuracy\n","        _, test_predicted = test_combined_logits.max(1)\n","        test_accuracy = (test_predicted == test_labels).sum().item() / test_labels.size(0)\n","        test_accuracies.append(test_accuracy)\n","\n","        # Store predicted and true labels for further evaluation\n","        predicted_labels.extend(test_predicted.cpu().numpy())\n","        true_labels.extend(test_labels.cpu().numpy())\n","\n","# Calculate evaluation metrics (e.g., precision, recall, F1-score)\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","test_accuracy = accuracy_score(true_labels, predicted_labels)\n","precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n","\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:44:57.226004Z","iopub.status.busy":"2024-06-18T05:44:57.225333Z","iopub.status.idle":"2024-06-18T05:44:58.002706Z","shell.execute_reply":"2024-06-18T05:44:58.001750Z","shell.execute_reply.started":"2024-06-18T05:44:57.225971Z"},"trusted":true},"outputs":[],"source":["test_losses = []\n","test_accuracies = []\n","predicted_labels = []\n","true_labels = []\n","\n","# Initialize a confusion matrix\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n","\n","# Test loop\n","with torch.no_grad():\n","    for test_images, test_texts, test_labels in test_loader:\n","        test_images = test_images.to(device)\n","        test_labels = test_labels.to(device)\n","\n","        test_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in test_texts]\n","        test_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in test_texts], dim=0).to(device)\n","        test_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in test_texts], dim=0).to(device)\n","\n","        test_img_feats = densenet121(test_images)\n","        test_img_feats = test_img_feats.squeeze()\n","\n","        test_outputs = bert_model(test_input_ids, attention_mask=test_attention_mask)\n","        test_text_feats = test_outputs.last_hidden_state[:, 0, :]\n","\n","        test_img_feats_reshaped = test_img_feats.view(test_img_feats.size(0), -1)  # Reshape test_img_feats\n","\n","        # Separate classifiers for image and text features\n","        test_img_classifier = torch.nn.Sequential(\n","            torch.nn.Linear(test_img_feats_reshaped.shape[1], 512).to(device),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(0.5),\n","            torch.nn.Linear(512, num_classes).to(device),\n","        )\n","\n","        test_text_classifier = torch.nn.Sequential(\n","            torch.nn.Linear(test_text_feats.shape[1], 512).to(device),\n","            torch.nn.ReLU(),\n","            torch.nn.Dropout(0.5),\n","            torch.nn.Linear(512, num_classes).to(device),\n","        )\n","\n","        # Get predictions for image and text modalities separately\n","        test_img_logits = test_img_classifier(test_img_feats_reshaped)\n","        test_text_logits = test_text_classifier(test_text_feats)\n","\n","        # Combine predictions using a fusion technique (e.g., simple averaging)\n","        test_combined_logits = 0.5 * (test_img_logits + test_text_logits)  # Simple averaging\n","\n","        # Ensure test_labels have the correct shape and type\n","        test_labels = test_labels.view(-1)  # Flatten test_labels to match batch size\n","        test_labels = test_labels.to(torch.long)  # Ensure test_labels are of type torch.long\n","\n","        # Check if test_labels are empty\n","        if test_labels.numel() == 0:\n","            print(f\"Skipping empty test labels batch\")\n","            continue\n","\n","        # Adjust the shape of test_combined_logits to match the batch size of test_labels\n","        test_combined_logits = test_combined_logits.view(test_labels.size(0), -1)\n","\n","        # Calculate test loss\n","        test_loss = criterion(test_combined_logits, test_labels)\n","        test_losses.append(test_loss.item())\n","\n","        # Calculate test accuracy\n","        _, test_predicted = test_combined_logits.max(1)\n","        test_accuracy = (test_predicted == test_labels).sum().item() / test_labels.size(0)\n","        test_accuracies.append(test_accuracy)\n","\n","        # Update confusion matrix\n","        conf_matrix += confusion_matrix(test_labels.cpu().numpy(), test_predicted.cpu().numpy(), labels=np.arange(num_classes))\n","\n","        # Store predicted and true labels for further evaluation\n","        predicted_labels.extend(test_predicted.cpu().numpy())\n","        true_labels.extend(test_labels.cpu().numpy())\n","\n","# Calculate evaluation metrics (e.g., precision, recall, F1-score)\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","test_accuracy = accuracy_score(true_labels, predicted_labels)\n","precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n","\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:45:01.552480Z","iopub.status.busy":"2024-06-18T05:45:01.551684Z","iopub.status.idle":"2024-06-18T05:45:01.991456Z","shell.execute_reply":"2024-06-18T05:45:01.990711Z","shell.execute_reply.started":"2024-06-18T05:45:01.552446Z"},"trusted":true},"outputs":[],"source":["# Display confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(num_classes), yticklabels=np.arange(num_classes))\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T05:45:18.303687Z","iopub.status.busy":"2024-06-18T05:45:18.303298Z","iopub.status.idle":"2024-06-18T05:45:18.704811Z","shell.execute_reply":"2024-06-18T05:45:18.703914Z","shell.execute_reply.started":"2024-06-18T05:45:18.303656Z"},"trusted":true},"outputs":[],"source":["# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","# Class names according to the label encoding mapping\n","class_names = ['Advocative', 'Controversial', 'ExhIbitionist', 'Expressive', 'Informative', 'Promotive']\n","sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5226942,"sourceId":8712717,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
