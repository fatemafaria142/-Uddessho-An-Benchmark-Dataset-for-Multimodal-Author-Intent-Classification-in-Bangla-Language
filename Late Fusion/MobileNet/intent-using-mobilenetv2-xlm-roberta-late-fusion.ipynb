{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-17T18:34:03.479280Z","iopub.status.busy":"2024-06-17T18:34:03.478882Z","iopub.status.idle":"2024-06-17T18:34:03.724282Z","shell.execute_reply":"2024-06-17T18:34:03.723270Z","shell.execute_reply.started":"2024-06-17T18:34:03.479250Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertTokenizer, BertModel, AdamW\n","import torchvision.models as models\n","from torch import nn\n","import time\n","from tqdm import tqdm\n","\n","# Paths to the CSV files and image directories\n","csv_paths = {\n","    'train': '/kaggle/input/intent/Intent/train.csv',\n","    'test': '/kaggle/input/intent/Intent/validation.csv',\n","    'validation': '/kaggle/input/intent/Intent/validation.csv'\n","}\n","\n","image_dirs = {\n","    'train': '/kaggle/input/intent/Intent/train',\n","    'test': '/kaggle/input/intent/Intent/validation',\n","    'validation': '/kaggle/input/intent/Intent/validation'\n","}\n","output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n","\n","# Function to check for matching Meme_ID and image files, and add image paths\n","def check_matches(csv_path, image_dir):\n","    df = pd.read_csv(csv_path)\n","    image_files = os.listdir(image_dir)\n","    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n","    \n","    # Add Image_Path column to the dataframe\n","    df['Image_Path'] = df['Image_ID'].apply(lambda x: image_names.get(x, None))\n","    \n","    # Filter rows where Image_Path is not None (i.e., matched Meme_IDs)\n","    matched_df = df[df['Image_Path'].notna()]\n","    \n","    return matched_df\n","\n","# Function to encode Intent_Taxonomy classes into labels\n","def encode_labels(df):\n","    label_encoder = LabelEncoder()\n","    df['Intent_Taxonomy_Labels'] = label_encoder.fit_transform(df['Intent_Taxonomy'])\n","    return df, label_encoder.classes_\n","\n","# Check matches for each set (Train, Test, Validation)\n","for key in csv_paths:\n","    matched_df = check_matches(csv_paths[key], image_dirs[key])\n","    \n","    # Encode Intent_Taxonomy labels\n","    matched_df, classes = encode_labels(matched_df)\n","    \n","    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n","    \n","    # Save the processed dataframe to CSV\n","    matched_df.to_csv(matches_output_path, index=False)\n","    \n","    print(f\"{key} set:\")\n","    print(f\"Matched Meme_IDs with image paths and labels saved to {matches_output_path}\")\n","    print(f\"Classes and their corresponding labels:\\n{dict(zip(classes, range(len(classes))))}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T18:34:04.495414Z","iopub.status.busy":"2024-06-17T18:34:04.495078Z","iopub.status.idle":"2024-06-17T18:34:04.510355Z","shell.execute_reply":"2024-06-17T18:34:04.509497Z","shell.execute_reply.started":"2024-06-17T18:34:04.495388Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/working/train_matches.csv')\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T18:34:05.070703Z","iopub.status.busy":"2024-06-17T18:34:05.070330Z","iopub.status.idle":"2024-06-17T18:34:05.085641Z","shell.execute_reply":"2024-06-17T18:34:05.084788Z","shell.execute_reply.started":"2024-06-17T18:34:05.070673Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/working/test_matches.csv')\n","test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T18:34:05.454464Z","iopub.status.busy":"2024-06-17T18:34:05.454114Z","iopub.status.idle":"2024-06-17T18:34:05.469202Z","shell.execute_reply":"2024-06-17T18:34:05.468335Z","shell.execute_reply.started":"2024-06-17T18:34:05.454436Z"},"trusted":true},"outputs":[],"source":["validation_df = pd.read_csv('/kaggle/working/validation_matches.csv')\n","validation_df.head()"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T18:34:05.836465Z","iopub.status.busy":"2024-06-17T18:34:05.836118Z","iopub.status.idle":"2024-06-17T18:34:05.845510Z","shell.execute_reply":"2024-06-17T18:34:05.844508Z","shell.execute_reply.started":"2024-06-17T18:34:05.836438Z"},"trusted":true},"outputs":[],"source":["# Define your transformations using transforms.Compose\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Crop the center to 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","class MyMultimodalDataset(Dataset):\n","    def __init__(self, image_paths, image_captions, intent_taxonomy_labels, transform=None):\n","        self.image_paths = image_paths\n","        self.image_captions = image_captions\n","        self.intent_taxonomy = intent_taxonomy_labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        text = self.image_captions[idx]\n","        label = self.intent_taxonomy[idx]\n","\n","        # Load and preprocess image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, text, label"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T18:34:06.316819Z","iopub.status.busy":"2024-06-17T18:34:06.316030Z","iopub.status.idle":"2024-06-17T18:34:06.324203Z","shell.execute_reply":"2024-06-17T18:34:06.323259Z","shell.execute_reply.started":"2024-06-17T18:34:06.316753Z"},"trusted":true},"outputs":[],"source":["# Assuming you have lists or arrays of image paths, captions, and encoded labels:\n","train_dataset = MyMultimodalDataset(train_df['Image_Path'], train_df['Image_Caption'], train_df['Intent_Taxonomy_Labels'], transform=transform)\n","val_dataset = MyMultimodalDataset(validation_df['Image_Path'], validation_df['Image_Caption'], validation_df['Intent_Taxonomy_Labels'], transform=transform)\n","test_dataset = MyMultimodalDataset(test_df['Image_Path'], test_df['Image_Caption'], test_df['Intent_Taxonomy_Labels'], transform=transform)\n","\n","# Define data loaders\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T18:35:54.735193Z","iopub.status.busy":"2024-06-17T18:35:54.734828Z","iopub.status.idle":"2024-06-17T18:35:56.937137Z","shell.execute_reply":"2024-06-17T18:35:56.936008Z","shell.execute_reply.started":"2024-06-17T18:35:54.735162Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import mobilenet_v2\n","from transformers import BertModel, BertTokenizer\n","from tqdm import tqdm\n","import time\n","\n","# Define device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# # Initialize BERT tokenizer and model\n","# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","# bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n","\n","# from transformers import AutoTokenizer, DistilBertModel, AdamW\n","\n","# # Initialize BERT tokenizer and model\n","# bert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","# bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n","\n","from transformers import AutoTokenizer, XLMRobertaModel, AdamW\n","# Initialize BERT tokenizer and model\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n","bert_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\").to(device)\n","\n","# Initialize MobileNetV2 (assuming it's your image feature extractor)\n","mobilenet_v2_model = mobilenet_v2(pretrained=True)\n","mobilenet_v2_model.classifier = nn.Identity()  # Remove the final classifier layer\n","mobilenet_v2_model.to(device)\n","mobilenet_v2_model.eval()  # Ensure in evaluation mode"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T18:35:57.034310Z","iopub.status.busy":"2024-06-17T18:35:57.034026Z","iopub.status.idle":"2024-06-17T18:36:00.165492Z","shell.execute_reply":"2024-06-17T18:36:00.164573Z","shell.execute_reply.started":"2024-06-17T18:35:57.034286Z"},"trusted":true},"outputs":[],"source":["# Define the combined classifier\n","class CombinedClassifier(nn.Module):\n","    def __init__(self, img_feature_dim, text_feature_dim, num_classes):\n","        super(CombinedClassifier, self).__init__()\n","        self.img_fc = nn.Sequential(\n","            nn.Linear(img_feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes)\n","        )\n","        self.text_fc = nn.Sequential(\n","            nn.Linear(text_feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, img_features, text_features):\n","        img_logits = self.img_fc(img_features)\n","        text_logits = self.text_fc(text_features)\n","        combined_logits = 0.5 * (img_logits + text_logits)  # Simple averaging\n","        return combined_logits\n","\n","\n","# Example usage assuming img_feature_dim is correctly set\n","combined_classifier = CombinedClassifier(img_feature_dim=1280, text_feature_dim=768, num_classes=6).to(device)\n","\n","# Define optimizer and criterion\n","optimizer = torch.optim.AdamW(list(mobilenet_v2_model.parameters()) + list(bert_model.parameters()) + list(combined_classifier.parameters()), lr=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Set number of epochs and other parameters\n","num_epochs = 40\n","max_seq_length = 100\n","\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","\n","start_time = time.time()\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    mobilenet_v2_model.train()\n","    bert_model.train()\n","    combined_classifier.train()\n","    \n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","        # Move tensors to the device\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Extract image features using MobileNetV2\n","        with torch.no_grad():\n","            img_feats = mobilenet_v2_model(images)\n","        \n","        # Reshape img_feats\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        # Convert texts to tensors and pad to a fixed sequence length\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        combined_logits = combined_classifier(img_feats, text_feats)\n","\n","        # Ensure the labels tensor is flattened to match the combined_logits batch size\n","        labels = labels.view(-1)\n","\n","        loss = criterion(combined_logits, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item()\n","        _, predicted = combined_logits.max(1)\n","        total_train += labels.size(0)\n","        correct_train += predicted.eq(labels).sum().item()\n","\n","    epoch_train_loss = running_train_loss / len(train_loader)\n","    epoch_train_accuracy = correct_train / total_train\n","\n","    train_losses.append(epoch_train_loss)\n","    train_accuracies.append(epoch_train_accuracy)\n","    \n","    # Validation loop\n","    mobilenet_v2_model.eval()\n","    bert_model.eval()\n","    combined_classifier.eval()\n","\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for val_images, val_texts, val_labels in val_loader:\n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","\n","            val_img_feats = mobilenet_v2_model(val_images)\n","            val_img_feats = val_img_feats.view(val_img_feats.size(0), -1)\n","\n","            val_texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in val_texts]\n","            val_input_ids = torch.stack([text['input_ids'].squeeze(0) for text in val_texts], dim=0).to(device)\n","            val_attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in val_texts], dim=0).to(device)\n","\n","            val_outputs = bert_model(val_input_ids, attention_mask=val_attention_mask)\n","            val_text_feats = val_outputs.last_hidden_state[:, 0, :]\n","\n","            val_combined_logits = combined_classifier(val_img_feats, val_text_feats)\n","\n","            # Ensure the val_labels tensor is flattened to match the val_combined_logits batch size\n","            val_labels = val_labels.view(-1)\n","\n","            val_loss = criterion(val_combined_logits, val_labels)\n","\n","            running_val_loss += val_loss.item()\n","            _, val_predicted = val_combined_logits.max(1)\n","            total_val += val_labels.size(0)\n","            correct_val += val_predicted.eq(val_labels).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_loader)\n","    epoch_val_accuracy = correct_val / total_val\n","\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_accuracy)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n","          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \"\n","          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n","\n","end_time = time.time()\n","execution_time = end_time - start_time\n","print(f\"Total execution time: {execution_time:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T18:36:00.167153Z","iopub.status.busy":"2024-06-17T18:36:00.166860Z","iopub.status.idle":"2024-06-17T18:36:00.661055Z","shell.execute_reply":"2024-06-17T18:36:00.660129Z","shell.execute_reply.started":"2024-06-17T18:36:00.167129Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","\n","# Assuming test_loader contains your test data loader\n","\n","combined_classifier.eval()  # Set the model to evaluation mode\n","\n","test_losses = []\n","test_accuracies = []\n","predictions = []\n","true_labels = []\n","\n","with torch.no_grad():\n","    for images, texts, labels in tqdm(test_loader, desc='Testing', leave=False):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        texts = [bert_tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt') for text in texts]\n","        input_ids = torch.stack([text['input_ids'].squeeze(0) for text in texts], dim=0).to(device)\n","        attention_mask = torch.stack([text['attention_mask'].squeeze(0) for text in texts], dim=0).to(device)\n","\n","        img_feats = mobilenet_v2_model(images)\n","        img_feats = img_feats.view(img_feats.size(0), -1)\n","\n","        outputs = bert_model(input_ids, attention_mask=attention_mask)\n","        text_feats = outputs.last_hidden_state[:, 0, :]\n","\n","        logits = combined_classifier(img_feats, text_feats)\n","\n","        # Calculate loss if needed\n","        test_loss = criterion(logits, labels)\n","        test_losses.append(test_loss.item())\n","\n","        # Calculate accuracy\n","        _, predicted = logits.max(1)\n","        predictions.extend(predicted.cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","        correct = predicted.eq(labels).sum().item()\n","        total = labels.size(0)\n","        test_accuracy = correct / total\n","        test_accuracies.append(test_accuracy)\n","\n","# Calculate overall test metrics\n","average_test_loss = sum(test_losses) / len(test_losses)\n","test_accuracy = accuracy_score(true_labels, predictions)\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_matrix = confusion_matrix(true_labels, predictions)\n","\n","# Print and plot results\n","print(f\"Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T18:36:01.309246Z","iopub.status.busy":"2024-06-17T18:36:01.308613Z","iopub.status.idle":"2024-06-17T18:36:01.723319Z","shell.execute_reply":"2024-06-17T18:36:01.722401Z","shell.execute_reply.started":"2024-06-17T18:36:01.309214Z"},"trusted":true},"outputs":[],"source":["# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","# Class names according to the label encoding mapping\n","class_names = ['Advocative', 'Controversial', 'ExhIbitionist', 'Expressive', 'Informative', 'Promotive']\n","sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5228393,"sourceId":8714678,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
